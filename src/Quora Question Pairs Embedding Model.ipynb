{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.hdf\n",
      "sample_submission.csv\n",
      "sample_submission.csv.zip\n",
      "test.csv\n",
      "test.csv.zip\n",
      "train.csv\n",
      "train.csv.zip\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avsolatorio/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from kaggle_quora_question_pairs_common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.27 s, sys: 320 ms, total: 6.59 s\n",
      "Wall time: 6.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df, test_df = load_train_test()\n",
    "unique_questions = get_unique_questions(train_df, test_df, include_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return text.str.replace('?', '').replace('.', '').str.replace('\\W', ' ').str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.9 s, sys: 964 ms, total: 29.9 s\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_questions = unique_questions.str.replace('?', '').replace('.', '').str.replace('\\W', ' ').str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_word2vec(\n",
    "    tokenized_questions,\n",
    "    pre_trained_model='GoogleNews-vectors-negative300.bin.gz',\n",
    "    size=300,\n",
    "    iter=20,\n",
    "    min_count=1,\n",
    "    negative=10,\n",
    "    workers=7,\n",
    "    min_alpha=0.0001,\n",
    "    window=5,\n",
    "    binary=True,\n",
    "):\n",
    "    # https://github.com/RaRe-Technologies/gensim/issues/1245\n",
    "    # List of tokenized questions.\n",
    "    # e.g. ['What', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india']\n",
    "    # pre_trained_model can be any pre trained model that gensim accepts, e.g., Glove or GoogleNews word2vec\n",
    "\n",
    "    # Initialize model\n",
    "    word_vectors = Word2Vec(\n",
    "        size=size, iter=iter, min_count=min_count, negative=negative, workers=workers,\n",
    "        min_alpha=min_alpha, window=window,\n",
    "    )\n",
    "\n",
    "    # Initialize vocab\n",
    "    word_vectors.build_vocab(tokenized_questions)\n",
    "\n",
    "    # Initialize vectors in local model with with vectors from pre-trained model with overlapping vocabulary.\n",
    "    # Set `lockf` to 1 for re-training\n",
    "    word_vectors.intersect_word2vec_format(pre_trained_model, lockf=1, binary=binary)\n",
    "\n",
    "    # Adjust pre-trained vectors to adapt its distribution with that of the local data via retraining.\n",
    "    word_vectors.train(tokenized_questions)\n",
    "\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 6min, sys: 1min 42s, total: 2h 7min 42s\n",
      "Wall time: 26min 8s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# start = datetime.now()\n",
    "# word_vectors = train_word2vec(tokenized_questions)\n",
    "# with open('word_vectors_done.lock', 'w') as fl:\n",
    "#     fl.write('{}'.format((datetime.now() - start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 14min 7s, sys: 1min 47s, total: 2h 15min 54s\n",
      "Wall time: 26min 50s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# start = datetime.now()\n",
    "# word_vectors_glove = train_word2vec(tokenized_questions, pre_trained_model='word2vec.glove.6B.300d.txt', binary=False)\n",
    "# with open('word_vectors_glove.lock', 'w') as fl:\n",
    "#     fl.write('{}'.format((datetime.now() - start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.21 s, sys: 388 ms, total: 3.6 s\n",
      "Wall time: 3.96 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # Basic cleaning, removal of . and ? and replacing of other non alnum with space.\n",
    "# word_vectors.save('word2vec_google_news_basic_cleaning.model')\n",
    "# word_vectors_glove.save('word2vec_glove_basic_cleaning.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vectors.wv.vocab) == len(word_vectors_glove.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 300)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.array([word_vectors[i] for i in st]).mean(axis=0) if st else np.zeros(300) for st in my_sentences[3285:3290]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 72 ms, sys: 0 ns, total: 72 ms\n",
      "Wall time: 72.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_word2vec_rep(model, raw_sentences):\n",
    "    tokenized_sentences = raw_sentences.str.replace('?', '').replace('.', '').str.replace('\\W', ' ').str.split()\n",
    "    return np.array(\n",
    "        [\n",
    "            np.array([model[i] for i in st if i in model.wv.vocab]).mean(axis=0) if st else \n",
    "            np.random.randn(model.vector_size) for st in tokenized_sentences\n",
    "        ]\n",
    "    )\n",
    "\n",
    "cos_dist = einsum_pairwise_cos_sim(\n",
    "    np.array([np.array([word_vectors[i] for i in st]).mean(axis=0) if st else np.random.randn(300) for st in tokenized_questions[:1000]]),\n",
    "    np.array([np.array([word_vectors[i] for i in st]).mean(axis=0) if st else np.random.randn(300) for st in tokenized_questions[1000:2000]]),\n",
    "#     binary_input=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print_question_pairs(train_df.head(20), sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 160 ms, sys: 0 ns, total: 160 ms\n",
      "Wall time: 157 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cos_dist_gn = einsum_pairwise_cos_sim(\n",
    "    get_word2vec_rep(word_vectors, train_df.head(1000).question1),\n",
    "    get_word2vec_rep(word_vectors, train_df.head(1000).question2)\n",
    ")\n",
    "\n",
    "cos_dist_glove = einsum_pairwise_cos_sim(\n",
    "    get_word2vec_rep(word_vectors_glove, train_df.head(1000).question1),\n",
    "    get_word2vec_rep(word_vectors_glove, train_df.head(1000).question2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.78194499,  0.77948368,  0.73246109,  0.78725821,  1.        ], dtype=float32),\n",
       " 205    0\n",
       " 206    0\n",
       " 207    0\n",
       " 208    0\n",
       " 209    1\n",
       " Name: is_duplicate, dtype: int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_dist_gn[205:210], train_df[205:210].is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(train_df.head(210).is_duplicate, np.nan_to_num(cos_dist_gn[:210]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.34524814],\n",
       "       [ 0.34524814,  1.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(cos_dist_glove, train_df.head(1000).is_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58339193749247253"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity('mail', 'email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('urllib', 0.5968994498252869),\n",
       " ('NumPy', 0.5179972648620605),\n",
       " ('urllib2', 0.5177142024040222),\n",
       " ('vpython', 0.5115652680397034),\n",
       " ('Ironpython', 0.4793962836265564),\n",
       " ('LAPACK', 0.47677332162857056),\n",
       " ('dotfiles', 0.46835094690322876),\n",
       " ('BCM94352', 0.46812641620635986),\n",
       " ('XPlanner', 0.46804100275039673),\n",
       " ('hashable', 0.4638808071613312)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar('numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# input_file     training file path (required)\n",
    "# output         output file path (required)\n",
    "# lr             learning rate [0.05]\n",
    "# lr_update_rate change the rate of updates for the learning rate [100]\n",
    "# dim            size of word vectors [100]\n",
    "# ws             size of the context window [5]\n",
    "# epoch          number of epochs [5]\n",
    "# min_count      minimal number of word occurences [5]\n",
    "# neg            number of negatives sampled [5]\n",
    "# word_ngrams    max length of word ngram [1]\n",
    "# loss           loss function {ns, hs, softmax} [ns]\n",
    "# bucket         number of buckets [2000000]\n",
    "# minn           min length of char ngram [3]\n",
    "# maxn           max length of char ngram [6]\n",
    "# thread         number of threads [12]\n",
    "# t              sampling threshold [0.0001]\n",
    "# silent         disable the log output from the C++ extension [1]\n",
    "# encoding       specify input_file encoding [utf-8]\n",
    "\n",
    "# Includes test data in unique_questions\n",
    "# pd.Series(unique_questions[:100000]).to_csv('questions_data_1000.csv', index=False)\n",
    "# NUM_PROC = 7\n",
    "# model = fasttext.skipgram('questions_data.csv', 'model_full_data', dim=300, epoch=30, thread=NUM_PROC, word_ngrams=2)\n",
    "# print model.words # list of words in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current all-time max memory: 136 MB\n",
      "Current all-time max memory: 3345 MB\n"
     ]
    }
   ],
   "source": [
    "log_max_mem_usage()\n",
    "model = fasttext.load_model('model_full_data.bin')\n",
    "log_max_mem_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.78911119]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(model[\"she is so India\"], model[\"she's so India\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.56863613]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"she's so beautiful\"\n",
    "s2 = \"she is pretty\"\n",
    "cosine_similarity(\n",
    "    np.sum([model[i] for i in s1.split()], axis=0),\n",
    "    np.sum([model[i] for i in s2.split()], axis=0),\n",
    "#     model[\"she's so beautiful\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_tfidf_lsi_models(tokenized_questions, num_topics=300, min_doc_freq=1, skip_terms=['']):\n",
    "    MIN_DOC_FREQ = min_doc_freq\n",
    "    NUM_TOPICS = num_topics\n",
    "\n",
    "    dictionary = (\n",
    "        gensim.corpora.Dictionary(\n",
    "            tokenized_questions\n",
    "        )\n",
    "    )\n",
    "\n",
    "    skip_terms = skip_terms\n",
    "\n",
    "    skip_ids = [dictionary.token2id[t] for t in skip_terms if t in dictionary.token2id]\n",
    "    low_freq_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq < MIN_DOC_FREQ]\n",
    "\n",
    "    dictionary.filter_tokens(low_freq_ids + skip_ids)\n",
    "    dictionary.compactify()\n",
    "\n",
    "    uid_corpus = (\n",
    "        {\n",
    "            uid: dictionary.doc2bow(\n",
    "                tq\n",
    "            ) for uid, tq in enumerate(tokenized_questions)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    tfidf = gensim.models.TfidfModel(uid_corpus.values(), id2word=dictionary)\n",
    "    lsi = gensim.models.LsiModel(tfidf[uid_corpus.values()], id2word=dictionary, num_topics=NUM_TOPICS)\n",
    "\n",
    "    return lsi, tfidf, uid_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ~4 hours to run\n",
    "# lsi, tfidf, uid_corpus = train_tfidf_lsi_models(tokenized_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# lsi.save('lsi_basic_cleaning.model')\n",
    "# tfidf.save('tfidf_basic_cleaning.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x7fdb6b980750>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi[tfidf[uid_corpus.values()[:2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lsi_transform(text, tfidf, lsi):\n",
    "    # text should be a dataframe row of string\n",
    "    db = [lsi.id2word.doc2bow(tokens) for tokens in tokenize_text(text)]\n",
    "    return np.array([zip(*c)[1] for c in lsi[tfidf[db]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 308 ms, sys: 12 ms, total: 320 ms\n",
      "Wall time: 305 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.30663122],\n",
       "       [ 0.30663122,  1.        ]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.corrcoef(\n",
    "    train_df.head(1000).is_duplicate,\n",
    "    einsum_pairwise_cos_sim(\n",
    "        lsi_transform(train_df.head(1000).question1, tfidf, lsi),\n",
    "        lsi_transform(train_df.head(1000).question2, tfidf, lsi)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tc = lsi_transform(train_df.head(10).question2, tfidf, lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in (30934, 0.1417583469020468)\n",
      "What (32845, 0.04838275493169238)\n",
      "guide (48775, 0.3832127702788639)\n",
      "step (48851, 0.6850161373260468)\n",
      "is (54136, 0.06737383155472171)\n",
      "india (75303, 0.247632307647888)\n",
      "by (105421, 0.18565958961538986)\n",
      "the (107952, 0.049689737134541064)\n",
      "invest (120282, 0.2923475709978697)\n",
      "to (125689, 0.07284383639019286)\n",
      "market (152422, 0.2739097420428594)\n",
      "share (159705, 0.3047377966227719)\n"
     ]
    }
   ],
   "source": [
    "for i in tfidf[[lsi.id2word.doc2bow(tokens) for tokens in tokenize_text(train_df.head(1).question1)]][0]:\n",
    "    print tfidf.id2word[i[0]], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4789032"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
