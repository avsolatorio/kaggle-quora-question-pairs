{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.hdf\n",
      "sample_submission.csv\n",
      "sample_submission.csv.zip\n",
      "test.csv\n",
      "test.csv.zip\n",
      "train.csv\n",
      "train.csv.zip\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avsolatorio/ml-ai/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from kaggle_quora_question_pairs_common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = load_train_test()\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current all-time max memory: 859 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(404290, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_max_mem_usage()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current all-time max memory: 13089 MB\n",
      "CPU times: user 976 ms, sys: 8 ms, total: 984 ms\n",
      "Wall time: 980 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "include_test = True\n",
    "unique_questions = get_unique_questions(train_df, test_df, include_test=include_test)\n",
    "\n",
    "log_max_mem_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "morphy_tag = {\n",
    "    'NN': wordnet.NOUN,\n",
    "    'JJ': wordnet.ADJ,\n",
    "    'VB': wordnet.VERB,\n",
    "    'VBN': wordnet.VERB,\n",
    "    'RB': wordnet.ADV,\n",
    "    'RBR': wordnet.ADV\n",
    "}\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag, default=wordnet.NOUN):\n",
    "#     http://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "#     http://stackoverflow.com/questions/7706696/how-can-i-best-determine-the-correct-capitalization-for-a-word\n",
    "    tag = default\n",
    "    \n",
    "    if treebank_tag.startswith('J'):\n",
    "        tag = wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        tag = wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        tag = wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        tag = wordnet.ADV\n",
    "    \n",
    "    return tag\n",
    "\n",
    "\n",
    "def get_lem_tag(tag):\n",
    "    return get_wordnet_pos(tag, default=wordnet.NOUN)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(word_tag):\n",
    "    word, tag = word_tag\n",
    "    tag = get_lem_tag(tag)\n",
    "    return lemmatizer.lemmatize(word, pos=tag)\n",
    "\n",
    "\n",
    "def lemmatize_words(words_tags):\n",
    "    lemmas = []\n",
    "    for word_tag in words_tags:\n",
    "        lemmas.append(lemmatize_word(word_tag))\n",
    "        \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = .str.replace(\"'\", '').str.replace('\\W', ' ').str.lower().dropna()\n",
    "lemmatized_corpus = corpus.str.split().map(lambda x: lemmatize_words(pos_tag(x)))\n",
    "\n",
    "words = [\n",
    "    word for response in lemmatized_corpus for word in response if word not in stops\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    'quoted': 'quoted_item',\n",
    "    'non-ascii': 'non_ascii_word',\n",
    "    'undefined': 'something'\n",
    "}\n",
    "\n",
    "def clean_string(text):\n",
    "    \n",
    "    def pad_str(s):\n",
    "        return ' ' + s + ' '\n",
    "    \n",
    "    # Empty question\n",
    "    \n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "\n",
    "    # preventing first and last word being ignored by regex    \n",
    "    text = ' ' + text + ' '\n",
    "\n",
    "    # Replace weird chars in text\n",
    "    \n",
    "    text = re.sub(\"’\", \"'\", text) # special single quote\n",
    "    text = re.sub(\"`\", \"'\", text) # special single quote\n",
    "    text = re.sub(\"“\", '\"', text) # special double quote\n",
    "    text = re.sub(\"？\", \"?\", text) \n",
    "    text = re.sub(\"…\", \" \", text) \n",
    "    text = re.sub(\"é\", \"e\", text) \n",
    "    \n",
    "    # Clean shorthands\n",
    "    \n",
    "    text = re.sub(\"\\'s \", \" is \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"can't\", \"can not\", text)\n",
    "    text = re.sub(\"n't\", \" not \", text)\n",
    "    text = re.sub(\" i'm \", \" I am \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\W|^)([0-9]+)[kK](\\W|$)\", r\"\\1\\g<2>000\\3\", text) # better regex provided by @armamut\n",
    "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n",
    "    \n",
    "#     # replace the float numbers with a random number, it will be parsed as number afterward, and also been replaced with word \"number\"\n",
    "    \n",
    "#     text = re.sub('[0-9]+\\.[0-9]+', \" 87 \", text)\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    \n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "#     # all numbers should separate from words, this is too aggressive\n",
    "    \n",
    "#     def pad_number(pattern):\n",
    "#         matched_string = pattern.group(0)\n",
    "#         return pad_str(matched_string)\n",
    "#     text = re.sub('[0-9]+', pad_number, text)\n",
    "    \n",
    "    # add padding to punctuations and special chars, we still need them later\n",
    "    \n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    \n",
    "    def pad_pattern(pattern):\n",
    "        matched_string = pattern.group(0)\n",
    "        return pad_str(matched_string)\n",
    "    text = re.sub('''[\\!\\?\\@\\^\\+\\*\\/\\,\\~\\|\\`\\=\\:\\;\\.\\#\\\\\\\\'\\\"]''', pad_pattern, text) \n",
    "        \n",
    "#     text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text) # replace non-ascii word with special word\n",
    "    text = text.decode('utf-8')\n",
    "    \n",
    "    # indian dollar\n",
    "    \n",
    "    text = re.sub(\"(?<=[0-9])rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\" rs(?=[0-9])\", \" rs \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # clean text rules get from : https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "    \n",
    "    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" india \", \" India \", text)\n",
    "    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n",
    "    text = re.sub(r\" china \", \" China \", text)\n",
    "    text = re.sub(r\" chinese \", \" Chinese \", text) \n",
    "    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dms \", \" direct messages \", text, flags=re.IGNORECASE)  \n",
    "    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" \\0rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gps \", \" GPS \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gst \", \" GST \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dna \", \" DNA \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" III \", \" 3 \", text)\n",
    "    text = re.sub(r\" banglore \", \" Banglore \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # typos identified with my eyes\n",
    "    \n",
    "    text = re.sub(r\" quikly \", \" quickly \", text)\n",
    "    text = re.sub(r\" unseccessful \", \" unsuccessful \", text)\n",
    "    text = re.sub(r\" demoniti[\\S]+ \", \" demonetization \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" demoneti[\\S]+ \", \" demonetization \", text, flags=re.IGNORECASE)  \n",
    "    text = re.sub(r\" addmision \", \" admission \", text)\n",
    "    text = re.sub(r\" insititute \", \" institute \", text)\n",
    "    text = re.sub(r\" connectionn \", \" connection \", text)\n",
    "    text = re.sub(r\" permantley \", \" permanently \", text)\n",
    "    text = re.sub(r\" sylabus \", \" syllabus \", text)\n",
    "    text = re.sub(r\" sequrity \", \" security \", text)\n",
    "    text = re.sub(r\" undergraduation \", \" undergraduate \", text) # not typo, but GloVe can't find it\n",
    "    text = re.sub(r\"(?=[a-zA-Z])ig \", \"ing \", text)\n",
    "    text = re.sub(r\" latop\", \" laptop\", text)\n",
    "    text = re.sub(r\" programmning \", \" programming \", text)  \n",
    "    text = re.sub(r\" begineer \", \" beginner \", text)  \n",
    "    text = re.sub(r\" qoura \", \" Quora \", text)\n",
    "    text = re.sub(r\" wtiter \", \" writer \", text)  \n",
    "    text = re.sub(r\" litrate \", \" literate \", text)  \n",
    "\n",
    "    # the single 's' in this stage is 99% of not clean text, just kill it\n",
    "    text = re.sub(' s ', \" \", text)\n",
    "    \n",
    "    # reduce extra spaces into single spaces\n",
    "    text = re.sub('[\\s]+', \" \", text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_tokenize_lemmatize(text, return_tokens=True):\n",
    "    tokens = lemmatize_words(pos_tag(word_tokenize(clean_string(text))))\n",
    "    \n",
    "    if not return_tokens:\n",
    "        tokens = ' '.join(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "958"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5000\n",
    "partitions = (unique_questions.shape[0] + batch_size) / batch_size\n",
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total expected partitions to process: 958\n",
      "Processed partition: 1\n",
      "Processed partition: 2\n",
      "Processed partition: 3\n",
      "Processed partition: 4\n",
      "Processed partition: 5\n",
      "Processed partition: 6\n",
      "Processed partition: 7\n",
      "Processed partition: 8\n",
      "Processed partition: 9\n",
      "Processed partition: 10\n",
      "Processed partition: 11\n",
      "Processed partition: 12\n",
      "Processed partition: 13\n",
      "Processed partition: 14\n",
      "Processed partition: 15\n",
      "Processed partition: 16\n",
      "Processed partition: 17\n",
      "Processed partition: 18\n",
      "Processed partition: 19\n",
      "Processed partition: 20\n",
      "Processed partition: 21\n",
      "Processed partition: 22\n",
      "Processed partition: 23\n",
      "Processed partition: 24\n",
      "Processed partition: 25\n",
      "Processed partition: 26\n",
      "Processed partition: 27\n",
      "Processed partition: 28\n",
      "Processed partition: 29\n",
      "Processed partition: 30\n",
      "Processed partition: 31\n",
      "Processed partition: 32\n",
      "Processed partition: 33\n",
      "Processed partition: 34\n",
      "Processed partition: 35\n",
      "Processed partition: 36\n",
      "Processed partition: 37\n",
      "Processed partition: 38\n",
      "Processed partition: 39\n",
      "Processed partition: 40\n",
      "Processed partition: 41\n",
      "Processed partition: 42\n",
      "Processed partition: 43\n",
      "Processed partition: 44\n",
      "Processed partition: 45\n",
      "Processed partition: 46\n",
      "Processed partition: 47\n",
      "Processed partition: 48\n",
      "Processed partition: 49\n",
      "Processed partition: 50\n",
      "Processed partition: 51\n",
      "Processed partition: 52\n",
      "Processed partition: 53\n",
      "Processed partition: 54\n",
      "Processed partition: 55\n",
      "Processed partition: 56\n",
      "Processed partition: 57\n",
      "Processed partition: 58\n",
      "Processed partition: 59\n",
      "Processed partition: 60\n",
      "Processed partition: 61\n",
      "Processed partition: 62\n",
      "Processed partition: 63\n",
      "Processed partition: 64\n",
      "Processed partition: 65\n",
      "Processed partition: 66\n",
      "Processed partition: 67\n",
      "Processed partition: 68\n",
      "Processed partition: 69\n",
      "Processed partition: 70\n",
      "Processed partition: 71\n",
      "Processed partition: 72\n",
      "Processed partition: 73\n",
      "Processed partition: 74\n",
      "Processed partition: 75\n",
      "Processed partition: 76\n",
      "Processed partition: 77\n",
      "Processed partition: 78\n",
      "Processed partition: 79\n",
      "Processed partition: 80\n",
      "Processed partition: 81\n",
      "Processed partition: 82\n",
      "Processed partition: 83\n",
      "Processed partition: 84\n",
      "Processed partition: 85\n",
      "Processed partition: 86\n",
      "Processed partition: 87\n",
      "Processed partition: 88\n",
      "Processed partition: 89\n",
      "Processed partition: 90\n",
      "Processed partition: 91\n",
      "Processed partition: 92\n",
      "Processed partition: 93\n",
      "Processed partition: 94\n",
      "Processed partition: 95\n",
      "Processed partition: 96\n",
      "Processed partition: 97\n",
      "Processed partition: 98\n",
      "Processed partition: 99\n",
      "Processed partition: 100\n",
      "Processed partition: 101\n",
      "Processed partition: 102\n",
      "Processed partition: 103\n",
      "Processed partition: 104\n",
      "Processed partition: 105\n",
      "Processed partition: 106\n",
      "Processed partition: 107\n",
      "Processed partition: 108\n",
      "Processed partition: 109\n",
      "Processed partition: 110\n",
      "Processed partition: 111\n",
      "Processed partition: 112\n",
      "Processed partition: 113\n",
      "Processed partition: 114\n",
      "Processed partition: 115\n",
      "Processed partition: 116\n",
      "Processed partition: 117\n",
      "Processed partition: 118\n",
      "Processed partition: 119\n",
      "Processed partition: 120\n",
      "Processed partition: 121\n",
      "Processed partition: 122\n",
      "Processed partition: 123\n",
      "Processed partition: 124\n",
      "Processed partition: 125\n",
      "Processed partition: 126\n",
      "Processed partition: 127\n",
      "Processed partition: 128\n",
      "Processed partition: 129\n",
      "Processed partition: 130\n",
      "Processed partition: 131\n",
      "Processed partition: 132\n",
      "Processed partition: 133\n",
      "Processed partition: 134\n",
      "Processed partition: 135\n",
      "Processed partition: 136\n",
      "Processed partition: 137\n",
      "Processed partition: 138\n",
      "Processed partition: 139\n",
      "Processed partition: 140\n",
      "Processed partition: 141\n",
      "Processed partition: 142\n",
      "Processed partition: 143\n",
      "Processed partition: 144\n",
      "Processed partition: 145\n",
      "Processed partition: 146\n",
      "Processed partition: 147\n",
      "Processed partition: 148\n",
      "Processed partition: 149\n",
      "Processed partition: 150\n",
      "Processed partition: 151\n",
      "Processed partition: 152\n",
      "Processed partition: 153\n",
      "Processed partition: 154\n",
      "Processed partition: 155\n",
      "Processed partition: 156\n",
      "Processed partition: 157\n",
      "Processed partition: 158\n",
      "Processed partition: 159\n",
      "Processed partition: 160\n",
      "Processed partition: 161\n",
      "Processed partition: 162\n",
      "Processed partition: 163\n",
      "Processed partition: 164\n",
      "Processed partition: 165\n",
      "Processed partition: 166\n",
      "Processed partition: 167\n",
      "Processed partition: 168\n",
      "Processed partition: 169\n",
      "Processed partition: 170\n",
      "Processed partition: 171\n",
      "Processed partition: 172\n",
      "Processed partition: 173\n",
      "Processed partition: 174\n",
      "Processed partition: 175\n",
      "Processed partition: 176\n",
      "Processed partition: 177\n",
      "Processed partition: 178\n",
      "Processed partition: 179\n",
      "Processed partition: 180\n",
      "Processed partition: 181\n",
      "Processed partition: 182\n",
      "Processed partition: 183\n",
      "Processed partition: 184\n",
      "Processed partition: 185\n",
      "Processed partition: 186\n",
      "Processed partition: 187\n",
      "Processed partition: 188\n",
      "Processed partition: 189\n",
      "Processed partition: 190\n",
      "Processed partition: 191\n",
      "Processed partition: 192\n",
      "Processed partition: 193\n",
      "Processed partition: 194\n",
      "Processed partition: 195\n",
      "Processed partition: 196\n",
      "Processed partition: 197\n",
      "Processed partition: 198\n",
      "Processed partition: 199\n",
      "Processed partition: 200\n",
      "Processed partition: 201\n",
      "Processed partition: 202\n",
      "Processed partition: 203\n",
      "Processed partition: 204\n",
      "Processed partition: 205\n",
      "Processed partition: 206\n",
      "Processed partition: 207\n",
      "Processed partition: 208\n",
      "Processed partition: 209\n",
      "Processed partition: 210\n",
      "Processed partition: 211\n",
      "Processed partition: 212\n",
      "Processed partition: 213\n",
      "Processed partition: 214\n",
      "Processed partition: 215\n",
      "Processed partition: 216\n",
      "Processed partition: 217\n",
      "Processed partition: 218\n",
      "Processed partition: 219\n",
      "Processed partition: 220\n",
      "Processed partition: 221\n",
      "Processed partition: 222\n",
      "Processed partition: 223\n",
      "Processed partition: 224\n",
      "Processed partition: 225\n",
      "Processed partition: 226\n",
      "Processed partition: 227\n",
      "Processed partition: 228\n",
      "Processed partition: 229\n",
      "Processed partition: 230\n",
      "Processed partition: 231\n",
      "Processed partition: 232\n",
      "Processed partition: 233\n",
      "Processed partition: 234\n",
      "Processed partition: 235\n",
      "Processed partition: 236\n",
      "Processed partition: 237\n",
      "Processed partition: 238\n",
      "Processed partition: 239\n",
      "Processed partition: 240\n",
      "Processed partition: 241\n",
      "Processed partition: 242\n",
      "Processed partition: 243\n",
      "Processed partition: 244\n",
      "Processed partition: 245\n",
      "Processed partition: 246\n",
      "Processed partition: 247\n",
      "Processed partition: 248\n",
      "Processed partition: 249\n",
      "Processed partition: 250\n",
      "Processed partition: 251\n",
      "Processed partition: 252\n",
      "Processed partition: 253\n",
      "Processed partition: 254\n",
      "Processed partition: 255\n",
      "Processed partition: 256\n",
      "Processed partition: 257\n",
      "Processed partition: 258\n",
      "Processed partition: 259\n",
      "Processed partition: 260\n",
      "Processed partition: 261\n",
      "Processed partition: 262\n",
      "Processed partition: 263\n",
      "Processed partition: 264\n",
      "Processed partition: 265\n",
      "Processed partition: 266\n",
      "Processed partition: 267\n",
      "Processed partition: 268\n",
      "Processed partition: 269\n",
      "Processed partition: 270\n",
      "Processed partition: 271\n",
      "Processed partition: 272\n",
      "Processed partition: 273\n",
      "Processed partition: 274\n",
      "Processed partition: 275\n",
      "Processed partition: 276\n",
      "Processed partition: 277\n",
      "Processed partition: 278\n",
      "Processed partition: 279\n",
      "Processed partition: 280\n",
      "Processed partition: 281\n",
      "Processed partition: 282\n",
      "Processed partition: 283\n",
      "Processed partition: 284\n",
      "Processed partition: 285\n",
      "Processed partition: 286\n",
      "Processed partition: 287\n",
      "Processed partition: 288\n",
      "Processed partition: 289\n",
      "Processed partition: 290\n",
      "Processed partition: 291\n",
      "Processed partition: 292\n",
      "Processed partition: 293\n",
      "Processed partition: 294\n",
      "Processed partition: 295\n",
      "Processed partition: 296\n",
      "Processed partition: 297\n",
      "Processed partition: 298\n",
      "Processed partition: 299\n",
      "Processed partition: 300\n",
      "Processed partition: 301\n",
      "Processed partition: 302\n",
      "Processed partition: 303\n",
      "Processed partition: 304\n",
      "Processed partition: 305\n",
      "Processed partition: 306\n",
      "Processed partition: 307\n",
      "Processed partition: 308\n",
      "Processed partition: 309\n",
      "Processed partition: 310\n",
      "Processed partition: 311\n",
      "Processed partition: 312\n",
      "Processed partition: 313\n",
      "Processed partition: 314\n",
      "Processed partition: 315\n",
      "Processed partition: 316\n",
      "Processed partition: 317\n",
      "Processed partition: 318\n",
      "Processed partition: 319\n",
      "Processed partition: 320\n",
      "Processed partition: 321\n",
      "Processed partition: 322\n",
      "Processed partition: 323\n",
      "Processed partition: 324\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print 'Total expected partitions to process: {}'.format(partitions)\n",
    "\n",
    "for ix, batch in enumerate(np.array_split(unique_questions, partitions)):\n",
    "    batch.map(lambda x: clean_tokenize_lemmatize(x, False)).to_csv(\n",
    "        'corpus/lemmatized-unnormalized-cleaned-data.with_test.csv', index=False, mode='a', sep='\\n',\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    print 'Processed partition: {}'.format(ix + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 972 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wvmodel = KeyedVectors.load_word2vec_format(\n",
    "    '/home/avsolatorio/WORK/kaggle/pre-trained-models/GoogleNews-vectors-negative300.bin.gz', binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = enchant.Dict('en-us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.check('bedraggled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
