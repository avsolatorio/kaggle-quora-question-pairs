{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": null,
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport resource\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n# ------- Define modular methods for the task\ndef log_max_mem_usage():\n    print(\n        \"Current all-time max memory: {} MB\".format(\n            resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1000\n        )\n    )",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "test.csv\ntrain.csv\n\n"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_df = pd.read_csv('../input/train.csv')\ntrain_df.dropna(inplace=True)  # For id: qid2 174364\n\ntrain_df.head(5)",
      "execution_count": 2,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n      <td>What would happen if the Indian government sto...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5</td>\n      <td>6</td>\n      <td>How can I increase the speed of my internet co...</td>\n      <td>How can Internet speed be increased by hacking...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>7</td>\n      <td>8</td>\n      <td>Why am I mentally very lonely? How can I solve...</td>\n      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>9</td>\n      <td>10</td>\n      <td>Which one dissolve in water quikly sugar, salt...</td>\n      <td>Which fish would survive in salt water?</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   id  qid1  qid2                                          question1  \\\n0   0     1     2  What is the step by step guide to invest in sh...   \n1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n2   2     5     6  How can I increase the speed of my internet co...   \n3   3     7     8  Why am I mentally very lonely? How can I solve...   \n4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n\n                                           question2  is_duplicate  \n0  What is the step by step guide to invest in sh...             0  \n1  What would happen if the Indian government sto...             0  \n2  How can Internet speed be increased by hacking...             0  \n3  Find the remainder when [math]23^{24}[/math] i...             0  \n4            Which fish would survive in salt water?             0  "
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "log_max_mem_usage()",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Current all-time max memory: 224.752 MB\n"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%%time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion\n\n# featurizers = [\n#     ('char_tfidf', TfidfVectorizer(analyzer='char', ngram_range=(2, 3))),\n#     ('word_tfidf', TfidfVectorizer(analyzer='word', ngram_range=(1, 2)))\n# ]\n\n# char_weight = 0.4\n# combined_featurizers = FeatureUnion(\n#     featurizers,\n#     n_jobs=7,\n#     transformer_weights={\n#         'char_tfidf': char_weight,\n#         'word_tfidf': 1 - char_weight\n#     }\n# )\n\nunique_questions = pd.Series(pd.concat([train_df.question1, train_df.question2]).unique())\n# combined_featurizers.fit(unique_questions)\n\nchar_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))  # featurizers[0][1]\nword_tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2))  # featurizers[1][1]\nchar_tfidf.fit(unique_questions)\nword_tfidf.fit(unique_questions)\n\nlog_max_mem_usage()",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Current all-time max memory: 1584.248 MB\nCPU times: user 1min 25s, sys: 1.61 s, total: 1min 26s\nWall time: 1min 24s\n"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    \ndef get_score(q1, q2):\n    qset = [q1, q2]\n    cv1, cv2 = char_tfidf.transform(qset)\n    wv1, wv2 = word_tfidf.transform(qset)\n    # cwv1, cwv2 = combined_featurizers.transform(qset)\n    cv=np.dot(cv1, cv2.T)\n    wv=np.dot(wv1, wv2.T)\n    \n    \n    return dict(\n        cv=np.dot(cv1, cv2.T).data[0] if cv else 0,\n        wv=np.dot(wv1, wv2.T).data[0] if wv else 0,\n        length_diff=abs(len(q1) - len(q2)),\n        word_num_diff=abs(len(q1.split()) - len(q2.split())),\n        # cwv=np.dot(cwv1, cwv2.T).data[0],\n    )\n\ndef score_row(row):\n    _, row = row\n    q1 = row.question1.lower()\n    q2 = row.question2.lower()\n    if not all([set(q1.split()).difference(stops), set(q2.split()).difference(stops)]):\n        return 0\n\n    return get_score(q1, q2)",
      "execution_count": 9,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%%time\nfrom sklearn.model_selection import train_test_split\n\nscores_data = []\nsamp = train_df\n\nfor row in samp.iterrows():\n    if row[0] % 5000 == 0:\n        print(row[0])\n\n    scores_data.append(score_row(row))\n\nfeatures = pd.DataFrame(scores_data)\n\nX_train, X_test, y_train, y_test = train_test_split(features, samp.is_duplicate, test_size=0.7)\nlog_max_mem_usage()",
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\n45000\n50000\n"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.linear_model.logistic import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss, roc_auc_score\n\nchar_lm_model = LogisticRegression(C=100)\nword_lm_model = LogisticRegression(C=1)\nlength_diff_lm_model = LogisticRegression(C=1)\nword_num_diff_lm_model = LogisticRegression(C=1)\nrf_model = RandomForestClassifier(n_estimators=100, min_samples_leaf=2, min_samples_split=3, n_jobs=-1)\n\ndef fit_models(X, y):\n    rf_model.fit(X_train, y_train)\n    char_lm_model.fit(X_train.cv.values.reshape(-1, 1), y_train)\n    word_lm_model.fit(X_train.wv.values.reshape(-1, 1), y_train)\n    length_diff_lm_model.fit(X_train.length_diff.values.reshape(-1, 1), y_train)\n    word_num_diff_lm_model.fit(X_train.word_num_diff.values.reshape(-1, 1), y_train)\n\ndef predict(X):\n    weights = dict(zip(X.columns, rf_model.feature_importances_))\n    char_pred = char_lm_model.predict_proba(X.cv.values.reshape(-1, 1))[:, 1] # * weights['cv']\n    word_pred = word_lm_model.predict_proba(X.wv.values.reshape(-1, 1))[:, 1] # * weights['wv']\n    length_diff_pred = length_diff_lm_model.predict_proba(X.length_diff.values.reshape(-1, 1))[:, 1] # * weights['length_diff']\n    word_num_diff_pred = word_num_diff_lm_model.predict_proba(X.word_num_diff.values.reshape(-1, 1))[:, 1] # * weights['word_num_diff']\n    rf_pred = rf_model.predict_proba(X)[:, 1]\n\n    return [char_pred, word_pred, length_diff_pred, word_num_diff_pred, rf_pred]",
      "execution_count": 192,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%%time\nfit_models(X_train, y_train)",
      "execution_count": 193,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "log_loss(y_test, np.mean(predict(X_test), axis=0))",
      "execution_count": 194,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1]))\n# log_loss(y_test, lm.predict_proba(X_test))\nsum((y_test - (1 * (rf_model.predict_proba(X_test)[:, 1] < 0.5))) != 0)",
      "execution_count": 195,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "list(zip(X_test.columns, rf_model.feature_importances_))",
      "execution_count": 196,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from IPython.display import FileLink\ntest_df = pd.read_csv('../input/test.csv')\n\nscores_data = []\nsamp = train_df\n\nfor row in samp.iterrows():\n    if row[0] % 5000 == 0:\n        print(row[0])\n\n    scores_data.append(score_row(row))\n\nX = pd.DataFrame(scores_data)\nis_duplicate_test = np.mean(predict(X), axis=0)\n\nlog_max_mem_usage()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for i in zip(y_test, (1 * (lm.predict_proba(X_test)[:, 1] < 0.45))):\n    print(i)",
      "execution_count": 132,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_df.ix[0].question1, train_df.ix[0].question2, train_df.ix[0].is_duplicate",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%%time\nX_train = combined_featurizers.transform(unique_questions)\n\ndef get_feature_vector(q, X, unique_q):\n    return X[np.where(unique_q == q)[0][0]]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%%time\nsamp = train_df.head(1000).copy()\nsamp['q1vec'] = samp.question1.map(lambda x: get_feature_vector(x, X_train, unique_questions))\nsamp['q2vec'] = samp.question2.map(lambda x: get_feature_vector(x, X_train, unique_questions))\n\nsamp.groupby('is_duplicate').count()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": null,
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%%time\npred = []\nfor i, j in zip(samp.q1vec, samp.q2vec):\n    pred.append(np.dot(i, j.T))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "roc_auc_score(samp.is_duplicate, pred)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": null,
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}